# -*- coding: utf-8 -*-
"""CCS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xAQO1bEb2UOw15zzXDBN4inIJOD6EfZv

# **Cloud Intrusion Detection Method Based on Stacked Contractive Auto-Encoder and Support Vector Machine**

## Mount drive
"""

from google.colab import drive
drive.mount('/content/drive')

"""## Install libraries"""

pip install tensorflow numpy pandas scikit-learn matplotlib seaborn

"""## Import modules"""

import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
import seaborn as sns
import itertools
import tensorflow.keras.backend as K
import joblib
import os

from tensorflow.keras.losses import Loss
from tensorflow.keras.utils import register_keras_serializable
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

from sklearn.preprocessing import StandardScaler
#from sklearn.svm import SVC
from sklearn.metrics import classification_report
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import RFE
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import roc_curve, auc
from sklearn.svm import LinearSVC

from pandas.api.types import is_numeric_dtype

from scipy.stats import zscore

from keras.saving import register_keras_serializable
from keras import backend as K

print(tf.__version__)

drive_path = "/content/drive/MyDrive/models"
os.makedirs(drive_path, exist_ok=True)

"""## Load dataset"""

df = pd.read_csv("/content/drive/MyDrive/arff dataset.csv")

df.shape

df.head()

"""## Data exploration"""

df.info()

df.describe()

"""## Preprocessing the dataset"""

#checking the dataset for null or missing values
total = df.shape[0]
missing_columns = [col for col in df.columns if df[col].isnull().sum() > 0]
for col in missing_columns:
    null_count = df[col].isnull().sum()
    per = (null_count/total) * 100
    print(f"{col}: {null_count} ({round(per, 3)}%)")

"""No null or missing values found in the dataset"""

print(f"Number of duplicate rows: {df.duplicated().sum()}")

"""No duplicate rows found in the dataset"""

categorical_features = ["protocol_type'", "service'", "flag'", "class'"]
encoders = {col: LabelEncoder() for col in categorical_features}

for col in categorical_features:
    df[col] = encoders[col].fit_transform(df[col])

df["class'"] = df["class'"].apply(lambda x: 0 if x == encoders["class'"].transform(['normal'])[0] else 1)

df["class'"].value_counts()

plt.figure(figsize=(40,30))
sns.heatmap(df.corr(), annot=True)

X_train = df.drop(["class'"], axis=1)
Y_train = df["class'"]

# Identify columns with low correlation to "class'"
correlation_matrix = df.corr()
class_correlation = correlation_matrix["class'"].abs().sort_values(ascending=False)
threshold = 0.05  #drop columns with correlation less than 0.05

columns_to_drop = class_correlation[class_correlation < threshold].index.tolist()
print("Columns to drop:", columns_to_drop)

"""Out of 39 features, 14 will be dropped due to low correlation with the attack type feature"""

# Drop the columns
df = df.drop(columns=columns_to_drop)

df.head()

"""Drop the attack type feature from the train dataset to divide it into X and Y. X being the input features and Y being the label"""

X_train = df.drop(["class'"], axis=1)
Y_train = df["class'"]

"""## Selecting the best features from the dataset to train the model

Recursive Feature Elimination is used to identify the best features to train the model on
"""

rfc = RandomForestClassifier()

rfe = RFE(rfc, n_features_to_select=10)
rfe = rfe.fit(X_train, Y_train)

feature_map = [(i, v) for i, v in itertools.zip_longest(rfe.get_support(), X_train.columns)]
selected_features = [v for i, v in feature_map if i==True]

selected_features

"""Replace the features in the dataset with the ones listed above"""

X_train = X_train[selected_features]

"""Split the dataset into train and test"""

x_train, x_test, y_train, y_test = train_test_split(X_train, Y_train, train_size=0.70, random_state=2)

"""Normalise the dataset"""

scaler = MinMaxScaler()
X_train = scaler.fit_transform(x_train)
X_test = scaler.transform(x_test)

"""## Building the stacked contractive auto-encoder model"""

@register_keras_serializable()
class ContractiveLoss(tf.keras.losses.Loss):
    def __init__(self, autoencoder, lam=1e-4, name="contractive_loss"):
        super().__init__(name=name)
        self.autoencoder = autoencoder
        self.lam = lam

    def call(self, y_true, y_pred):
        encoder_layer = self.autoencoder.get_layer(index=1)  # First Dense layer (128 units)
        W = encoder_layer.weights[0]
        W = tf.transpose(W)

        h = encoder_layer(y_pred)
        dh = tf.cast(h > 0, tf.float32)  # Derivative of ReLU

        contractive_penalty = self.lam * tf.reduce_sum(dh ** 2 * tf.reduce_sum(W ** 2, axis=1))
        return tf.reduce_mean(tf.square(y_true - y_pred)) + contractive_penalty


# Step 1: Build Autoencoder
input_dim = X_train.shape[1]
encoding_dim = 16

input_layer = tf.keras.Input(shape=(input_dim,))
encoded = tf.keras.layers.Dense(128, activation='relu')(input_layer)
encoded = tf.keras.layers.Dense(encoding_dim, activation='relu')(encoded)

decoded = tf.keras.layers.Dense(28, activation='relu')(encoded)
decoded = tf.keras.layers.Dense(input_dim, activation='linear')(decoded)

autoencoder = tf.keras.Model(inputs=input_layer, outputs=decoded)

# Step 2: Compile with custom loss AFTER building the model
loss_fn = ContractiveLoss(autoencoder)
autoencoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005), loss=loss_fn)

# Step 3: Train
early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)

history = autoencoder.fit(x_train, x_train,
                          epochs=50,
                          batch_size=32,
                          validation_data=(x_test, x_test),
                          callbacks=[early_stopping])

"""Understanding Each Part
Epoch n/50 ‚Üí This is the nth iteration of training, out of 50 total epochs.

2756/2756 ‚Üí The dataset is split into 2756 batches per epoch.

xs yms/step ‚Üí Each training step takes about y milliseconds, and the full epoch takes x seconds.

loss ‚Üí training loss

val_loss ‚Üí validation loss

Extract features
"""

encoder = Model(inputs=autoencoder.input, outputs=autoencoder.get_layer(index=2).output)
X_train_encoded = encoder.predict(x_train)
X_test_encoded = encoder.predict(x_test)

"""## Training the SVM"""

# Use LinearSVC for better scalability
svm = LinearSVC(C=1, dual=False, max_iter=5000)  # Increase max_iter if needed
svm.fit(X_train_encoded, y_train)

# Predictions
y_pred = svm.predict(X_test_encoded)

# Evaluate Model
print(classification_report(y_test, y_pred))

"""## Downloading .h5 file"""

print(autoencoder.loss)  # Should print something like: <__main__.ContractiveLoss object ...>

# Save Autoencoder in Keras format
autoencoder.save("scae_model.keras")

# Or, for TensorFlow Serving (optional, not both):
# autoencoder.export("scae_model_tf")

# Save Encoder
encoder.save(os.path.join(drive_path, "encoder_model.h5"))

# Save SVM Model
joblib.dump(svm, os.path.join(drive_path, "svm_model.pkl"))

print("Models saved to Google Drive successfully! üöÄ")

"""## Features and their meaning

üîç Feature Descriptions
Feature	Meaning
1. Protocol Type	Network protocol used in the connection:
‚Ä¢ 0 = TCP, 1 = UDP, 2 = ICMP (after label encoding).
2. Service	The network service on the destination (e.g., HTTP, FTP, SMTP). After encoding, it‚Äôs numeric.
3. Flag	Status flag of the connection (e.g., SF = successful, REJ = rejected). Encoded numerically.
4. Logged In	Whether the connection is from a logged-in user.
‚Ä¢ 1 = yes, 0 = no.
5. Count	Number of connections from the same source IP in the past 2 seconds. Indicates burst behavior.
6. Same Srv Rate	Percentage of connections to the same service. Shows if the host is targeting a specific port.
7. Diff Srv Rate	Percentage of connections to different services. High value may indicate scanning activity.
8. Dst Host Srv Count	Number of connections to the same service, to the same destination host. High values = target focus.
9. Dst Host Same Srv Rate	Ratio of connections with the same service to the total connections with that destination host.
10. Dst Host Same Src Port Rate	Ratio of connections to the same destination host and same source port.
High values could indicate a script or bot using a fixed source port.

Intrusion
1.   Protocol Type: 1
2. Service: 25
3. Flag: 2
4. Logged In: 0
5. Count: 500
6. Same Srv Rate: 0.02
7. Diff Srv Rate: 0.9
8. Dst Host Srv Count: 255
9. Dst Host Same Srv Rate: 0.03
10. Dst Host Same Src Port Rate: 0.1

Normal
1.   Protocol Type: 0
2. Service: 2
3. Flag: 0
4. Logged In: 1
5. Count: 5
6. Same Srv Rate: 0.9
7. Diff Srv Rate: 0.05
8. Dst Host Srv Count: 30
9. Dst Host Same Srv Rate: 0.95
10. Dst Host Same Src Port Rate: 0.85
"""